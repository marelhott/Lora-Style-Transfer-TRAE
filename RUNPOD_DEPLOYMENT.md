# ğŸš€ RunPod Deployment Guide

## PÅ™ehled

Tento guide popisuje deployment LoRA Style Transfer aplikace na RunPod s AMD64 architekturou a GPU optimalizacemi.

## ğŸ“‹ PoÅ¾adavky

### Hardware
- **GPU**: NVIDIA RTX 4090, Tesla V100, A100 nebo podobnÃ¡ (min. 12GB VRAM)
- **RAM**: MinimÃ¡lnÄ› 16GB, doporuÄeno 32GB+
- **Storage**: 200GB+ pro modely a LoRA
- **Architektura**: AMD64/x86_64

### Software
- Docker
- NVIDIA Container Toolkit
- CUDA 11.8+

## ğŸ”§ PÅ™Ã­prava

### 1. PÅ™Ã­prava modelÅ¯

Nahrajte svÃ© modely do persistentnÃ­ho ÃºloÅ¾iÅ¡tÄ›:

```bash
# Stable Diffusion modely (.safetensors, .ckpt)
/data/models/
â”œâ”€â”€ stable-diffusion-v1-5.safetensors
â”œâ”€â”€ realistic-vision-v5.safetensors
â””â”€â”€ dreamshaper-v8.safetensors

# LoRA modely (.safetensors, .pt)
/data/loras/
â”œâ”€â”€ portrait-enhancer.safetensors
â”œâ”€â”€ anime-style.safetensors
â””â”€â”€ landscape-master.safetensors
```

### 2. Build Docker Image

```bash
# Clone repository
git clone https://github.com/your-username/lora-style-transfer.git
cd lora-style-transfer

# Build image (nebo pouÅ¾ij hotovÃ½)
docker build -t lora-style-transfer:latest .

# NEBO pouÅ¾ij hotovÃ½ image z Docker Hub
docker pull mulenmara1505/lora-style-transfer:fullstack

# Tag pro registry
docker tag lora-style-transfer:latest your-registry/lora-style-transfer:latest

# Push do registry
docker push your-registry/lora-style-transfer:latest
```

## ğŸš€ Quick Start s hotovÃ½m image

```bash
# SpuÅ¡tÄ›nÃ­ s hotovÃ½m Docker image
docker run -d \
  --name lora-style-transfer \
  --gpus all \
  -p 3000:3000 \
  -p 8000:8000 \
  -v /workspace:/data \
  mulenmara1505/lora-style-transfer:fullstack
```

### RunPod Template

1. **Image**: `mulenmara1505/lora-style-transfer:fullstack`
2. **Ports**: `3000/http` (frontend), `8000/http` (backend API)
3. **Volume**: `/workspace` â†’ `/data` (persistent storage)
4. **GPU**: RTX 4090 nebo lepÅ¡Ã­ (min. 12GB VRAM)

## ğŸ› Troubleshooting

### Failed to fetch chyba

**ProblÃ©m**: Frontend nemÅ¯Å¾e volat backend API na RunPod proxy URL.

**Å˜eÅ¡enÃ­**:

1. **AutomatickÃ¡ detekce URL** (implementovÃ¡no):
   - Frontend automaticky detekuje RunPod proxy pattern
   - Pattern: `xxx-3000.proxy.runpod.net` â†’ `xxx-8000.proxy.runpod.net`

2. **ManuÃ¡lnÃ­ nastavenÃ­** v Backend Settings:
   ```
   https://your-runpod-id-8000.proxy.runpod.net
   ```

3. **Debug v browser console**:
   ```javascript
   // Zkontroluj debug logy
   console.log('ğŸ”§ getApiBaseUrl() called')
   console.log('ğŸ”§ Current host:', window.location.hostname)
   console.log('ğŸ” Loading models from:', apiUrl)
   ```

### Backend nedostupnÃ½

**Kontrola**:
```bash
# V RunPod terminÃ¡lu
curl http://localhost:8000/api/health

# Zkontroluj procesy
ps aux | grep uvicorn

# Zkontroluj porty
ss -tulpn | grep 8000
```

**Restart**:
```bash
# Restart containeru
docker restart <container_id>

# Nebo manuÃ¡lnÃ­ spuÅ¡tÄ›nÃ­
cd /app/backend
python main.py
```

## ğŸš€ Deployment na RunPod

### Metoda 1: RunPod Template (DoporuÄeno)

1. **VytvoÅ™te Template**:
   ```bash
   # Upload runpod-config.yaml do RunPod
   runpod template create --config runpod-config.yaml
   ```

2. **SpusÅ¥te Pod**:
   - Vyberte template "lora-style-transfer"
   - Zvolte GPU (RTX 4090 nebo lepÅ¡Ã­)
   - Nastavte persistent storage
   - SpusÅ¥te pod

### Metoda 2: ManuÃ¡lnÃ­ Setup

1. **VytvoÅ™te novÃ½ Pod**:
   - Image: `your-registry/lora-style-transfer:latest`
   - GPU: RTX 4090 nebo lepÅ¡Ã­
   - RAM: 16GB+
   - Storage: 200GB+

2. **Nastavte Volume Mounts**:
   ```
   /data/models -> Persistent storage pro modely
   /data/loras -> Persistent storage pro LoRA
   /tmp/processing -> Temporary storage
   ```

3. **Environment Variables**:
   ```bash
   CUDA_VISIBLE_DEVICES=0
   PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
   OMP_NUM_THREADS=4
   NEXT_PUBLIC_API_URL=http://localhost:8000
   ```

4. **SpouÅ¡tÄ›cÃ­ pÅ™Ã­kaz**:
   ```bash
   /app/docker-entrypoint.sh backend
   ```

## ğŸ” TestovÃ¡nÃ­

### 1. Health Check

```bash
# Kontrola API
curl http://your-pod-ip:8000/api/health

# OÄekÃ¡vanÃ¡ odpovÄ›Ä:
{
  "status": "healthy",
  "gpu_info": {
    "cuda_available": true,
    "device_name": "NVIDIA RTX 4090"
  },
  "models_available": 3
}
```

### 2. Test modelÅ¯

```bash
# Seznam dostupnÃ½ch modelÅ¯
curl http://your-pod-ip:8000/api/models

# Test zpracovÃ¡nÃ­
curl -X POST http://your-pod-ip:8000/api/process \
  -F "image=@test-image.jpg" \
  -F "model_id=model_stable-diffusion-v1-5" \
  -F 'parameters={"strength":0.8,"steps":20}'
```

### 3. SystÃ©movÃ½ test

```bash
# SpusÅ¥ v containeru
docker exec -it your-container /app/docker-entrypoint.sh test
```

## ğŸ“Š Monitoring

### GPU Utilization

```bash
# V containeru
nvidia-smi -l 1

# Nebo pÅ™es API
curl http://your-pod-ip:8000/api/health | jq '.gpu_info'
```

### Memory Usage

```bash
# CelkovÃ¡ pamÄ›Å¥
free -h

# GPU pamÄ›Å¥
nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

### Logs

```bash
# Backend logs
docker logs your-container

# Real-time logs
docker logs -f your-container
```

## âš¡ Optimalizace

### 1. GPU Memory

```python
# V kÃ³du jsou jiÅ¾ implementovÃ¡ny:
- CPU offload pro Ãºsporu VRAM
- Attention slicing
- Model caching
- Memory cleanup
```

### 2. Performance Tuning

```bash
# Environment variables pro optimalizaci
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export OMP_NUM_THREADS=4
export CUDA_LAUNCH_BLOCKING=0
```

### 3. Model Quantization

```python
# PovolenÃ­ v kÃ³du:
- FP16 precision
- Model quantization (volitelnÃ©)
- XFormers attention (pokud dostupnÃ©)
```

## ğŸ› Troubleshooting

### ÄŒastÃ© problÃ©my

1. **CUDA Out of Memory**:
   ```bash
   # SniÅ¾te batch size nebo povolte CPU offload
   # Zkontrolujte PYTORCH_CUDA_ALLOC_CONF
   ```

2. **Modely se nenaÄÃ­tajÃ­**:
   ```bash
   # Zkontrolujte cesty
   ls -la /data/models/
   ls -la /data/loras/
   
   # Zkontrolujte oprÃ¡vnÄ›nÃ­
   chmod -R 755 /data/
   ```

3. **PomalÃ© zpracovÃ¡nÃ­**:
   ```bash
   # Zkontrolujte GPU utilization
   nvidia-smi
   
   # Povolte XFormers
   pip install xformers
   ```

### Debug Mode

```bash
# SpusÅ¥ s debug logovÃ¡nÃ­m
docker run -e LOG_LEVEL=debug your-image

# Nebo v containeru
export LOG_LEVEL=debug
python -c "import logging; logging.basicConfig(level=logging.DEBUG)"
```

## ğŸ“ˆ Scaling

### Multi-GPU Setup

```yaml
# V runpod-config.yaml
resources:
  requests:
    nvidia.com/gpu: 2  # VÃ­ce GPU
  limits:
    nvidia.com/gpu: 2
```

### Load Balancing

```bash
# SpusÅ¥te vÃ­ce instancÃ­
# PouÅ¾ijte load balancer pÅ™ed API
```

## ğŸ”’ Security

### API Security

```python
# PÅ™idejte API klÃ­Äe
# Implementujte rate limiting
# PouÅ¾ijte HTTPS
```

### Network Security

```bash
# Omezte pÅ™Ã­stup k portÅ¯m
# PouÅ¾ijte VPN pro pÅ™Ã­stup
```

## ğŸ“ Support

Pro podporu a dotazy:
- GitHub Issues: [repository-url]
- Email: your-email@domain.com
- Discord: [discord-link]

## ğŸ“„ License

MIT License - viz LICENSE soubor