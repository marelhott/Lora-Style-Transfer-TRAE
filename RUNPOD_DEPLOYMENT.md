# üöÄ RunPod Deployment Guide

## P≈ôehled

Tento guide popisuje deployment LoRA Style Transfer aplikace na RunPod s AMD64 architekturou a GPU optimalizacemi.

## üìã Po≈æadavky

### Hardware
- **GPU**: NVIDIA RTX 4090, Tesla V100, A100 nebo podobn√° (min. 12GB VRAM)
- **RAM**: Minim√°lnƒõ 16GB, doporuƒçeno 32GB+
- **Storage**: 200GB+ pro modely a LoRA
- **Architektura**: AMD64/x86_64

### Software
- Docker
- NVIDIA Container Toolkit
- CUDA 11.8+

## üîß P≈ô√≠prava

### 1. P≈ô√≠prava model≈Ø

Nahrajte sv√© modely do persistentn√≠ho √∫lo≈æi≈°tƒõ:

```bash
# Stable Diffusion modely (.safetensors, .ckpt)
/data/models/
‚îú‚îÄ‚îÄ stable-diffusion-v1-5.safetensors
‚îú‚îÄ‚îÄ realistic-vision-v5.safetensors
‚îî‚îÄ‚îÄ dreamshaper-v8.safetensors

# LoRA modely (.safetensors, .pt)
/data/loras/
‚îú‚îÄ‚îÄ portrait-enhancer.safetensors
‚îú‚îÄ‚îÄ anime-style.safetensors
‚îî‚îÄ‚îÄ landscape-master.safetensors
```

### 2. Build Docker Image

```bash
# Clone repository
git clone https://github.com/your-username/lora-style-transfer.git
cd lora-style-transfer

# Build image
docker build -t lora-style-transfer:latest .

# Tag pro registry
docker tag lora-style-transfer:latest your-registry/lora-style-transfer:latest

# Push do registry
docker push your-registry/lora-style-transfer:latest
```

## üöÄ Deployment na RunPod

### Metoda 1: RunPod Template (Doporuƒçeno)

1. **Vytvo≈ôte Template**:
   ```bash
   # Upload runpod-config.yaml do RunPod
   runpod template create --config runpod-config.yaml
   ```

2. **Spus≈•te Pod**:
   - Vyberte template "lora-style-transfer"
   - Zvolte GPU (RTX 4090 nebo lep≈°√≠)
   - Nastavte persistent storage
   - Spus≈•te pod

### Metoda 2: Manu√°ln√≠ Setup

1. **Vytvo≈ôte nov√Ω Pod**:
   - Image: `your-registry/lora-style-transfer:latest`
   - GPU: RTX 4090 nebo lep≈°√≠
   - RAM: 16GB+
   - Storage: 200GB+

2. **Nastavte Volume Mounts**:
   ```
   /data/models -> Persistent storage pro modely
   /data/loras -> Persistent storage pro LoRA
   /tmp/processing -> Temporary storage
   ```

3. **Environment Variables**:
   ```bash
   CUDA_VISIBLE_DEVICES=0
   PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
   OMP_NUM_THREADS=4
   NEXT_PUBLIC_API_URL=http://localhost:8000
   ```

4. **Spou≈°tƒõc√≠ p≈ô√≠kaz**:
   ```bash
   /app/docker-entrypoint.sh backend
   ```

## üîç Testov√°n√≠

### 1. Health Check

```bash
# Kontrola API
curl http://your-pod-ip:8000/api/health

# Oƒçek√°van√° odpovƒõƒè:
{
  "status": "healthy",
  "gpu_info": {
    "cuda_available": true,
    "device_name": "NVIDIA RTX 4090"
  },
  "models_available": 3
}
```

### 2. Test model≈Ø

```bash
# Seznam dostupn√Ωch model≈Ø
curl http://your-pod-ip:8000/api/models

# Test zpracov√°n√≠
curl -X POST http://your-pod-ip:8000/api/process \
  -F "image=@test-image.jpg" \
  -F "model_id=model_stable-diffusion-v1-5" \
  -F 'parameters={"strength":0.8,"steps":20}'
```

### 3. Syst√©mov√Ω test

```bash
# Spus≈• v containeru
docker exec -it your-container /app/docker-entrypoint.sh test
```

## üìä Monitoring

### GPU Utilization

```bash
# V containeru
nvidia-smi -l 1

# Nebo p≈ôes API
curl http://your-pod-ip:8000/api/health | jq '.gpu_info'
```

### Memory Usage

```bash
# Celkov√° pamƒõ≈•
free -h

# GPU pamƒõ≈•
nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

### Logs

```bash
# Backend logs
docker logs your-container

# Real-time logs
docker logs -f your-container
```

## ‚ö° Optimalizace

### 1. GPU Memory

```python
# V k√≥du jsou ji≈æ implementov√°ny:
- CPU offload pro √∫sporu VRAM
- Attention slicing
- Model caching
- Memory cleanup
```

### 2. Performance Tuning

```bash
# Environment variables pro optimalizaci
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export OMP_NUM_THREADS=4
export CUDA_LAUNCH_BLOCKING=0
```

### 3. Model Quantization

```python
# Povolen√≠ v k√≥du:
- FP16 precision
- Model quantization (voliteln√©)
- XFormers attention (pokud dostupn√©)
```

## üêõ Troubleshooting

### ƒåast√© probl√©my

1. **CUDA Out of Memory**:
   ```bash
   # Sni≈æte batch size nebo povolte CPU offload
   # Zkontrolujte PYTORCH_CUDA_ALLOC_CONF
   ```

2. **Modely se nenaƒç√≠taj√≠**:
   ```bash
   # Zkontrolujte cesty
   ls -la /data/models/
   ls -la /data/loras/
   
   # Zkontrolujte opr√°vnƒõn√≠
   chmod -R 755 /data/
   ```

3. **Pomal√© zpracov√°n√≠**:
   ```bash
   # Zkontrolujte GPU utilization
   nvidia-smi
   
   # Povolte XFormers
   pip install xformers
   ```

### Debug Mode

```bash
# Spus≈• s debug logov√°n√≠m
docker run -e LOG_LEVEL=debug your-image

# Nebo v containeru
export LOG_LEVEL=debug
python -c "import logging; logging.basicConfig(level=logging.DEBUG)"
```

## üìà Scaling

### Multi-GPU Setup

```yaml
# V runpod-config.yaml
resources:
  requests:
    nvidia.com/gpu: 2  # V√≠ce GPU
  limits:
    nvidia.com/gpu: 2
```

### Load Balancing

```bash
# Spus≈•te v√≠ce instanc√≠
# Pou≈æijte load balancer p≈ôed API
```

## üîí Security

### API Security

```python
# P≈ôidejte API kl√≠ƒçe
# Implementujte rate limiting
# Pou≈æijte HTTPS
```

### Network Security

```bash
# Omezte p≈ô√≠stup k port≈Øm
# Pou≈æijte VPN pro p≈ô√≠stup
```

## üìû Support

Pro podporu a dotazy:
- GitHub Issues: [repository-url]
- Email: your-email@domain.com
- Discord: [discord-link]

## üìÑ License

MIT License - viz LICENSE soubor