#!/bin/bash

# RunPod Docker Entrypoint Script
# SpouÅ¡tÃ­ backend API a frontend souÄasnÄ›

set -e

echo "ğŸš€ Starting LoRA Style Transfer on RunPod..."

# Kontrola GPU dostupnosti
if command -v nvidia-smi &> /dev/null; then
    echo "ğŸ“Š GPU Information:"
    nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits
else
    echo "âš ï¸  Warning: nvidia-smi not found, GPU may not be available"
fi

# Kontrola CUDA
if python -c "import torch; print(f'ğŸ”¥ CUDA Available: {torch.cuda.is_available()}')" 2>/dev/null; then
    python -c "import torch; print(f'ğŸ¯ CUDA Device: {torch.cuda.get_device_name() if torch.cuda.is_available() else "None"}')"
else
    echo "âš ï¸  Warning: Could not check CUDA availability"
fi

# Kontrola dostupnosti modelÅ¯ a LoRA
echo "ğŸ“ Checking data directories..."
if [ -d "/data/models" ]; then
    MODEL_COUNT=$(find /data/models -name "*.safetensors" -o -name "*.ckpt" | wc -l)
    echo "ğŸ“¦ Found $MODEL_COUNT Stable Diffusion models in /data/models"
else
    echo "âš ï¸  Warning: /data/models directory not found"
    mkdir -p /data/models
fi

if [ -d "/data/loras" ]; then
    LORA_COUNT=$(find /data/loras -name "*.safetensors" -o -name "*.pt" | wc -l)
    echo "ğŸ¨ Found $LORA_COUNT LoRA models in /data/loras"
else
    echo "âš ï¸  Warning: /data/loras directory not found"
    mkdir -p /data/loras
fi

# VytvoÅ™enÃ­ temp adresÃ¡Å™Å¯
mkdir -p /tmp/processing

# NastavenÃ­ environment variables
export PYTHONPATH="/app/backend:$PYTHONPATH"
export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Optimalizace pro RunPod
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4
export NUMEXPR_NUM_THREADS=4

# Funkce pro spuÅ¡tÄ›nÃ­ backend API
start_backend() {
    echo "ğŸ”§ Starting Backend API on port 8000..."
    cd /app/backend
    
    # Kontrola zÃ¡vislostÃ­
    python -c "import torch, diffusers, transformers; print('âœ… All AI dependencies loaded successfully')"
    
    # SpuÅ¡tÄ›nÃ­ s optimalizacemi
    exec uvicorn main:app \
        --host 0.0.0.0 \
        --port 8000 \
        --workers 1 \
        --log-level info \
        --access-log \
        --use-colors
}

# Funkce pro spuÅ¡tÄ›nÃ­ frontend (volitelnÃ©)
start_frontend() {
    echo "ğŸŒ Starting Frontend on port 3000..."
    cd /app
    
    # NastavenÃ­ environment pro frontend - nechej automatickou detekci
    # export NEXT_PUBLIC_API_URL="http://localhost:8000"  # ZakÃ¡zÃ¡no - pouÅ¾Ã­vÃ¡ se automatickÃ¡ detekce
    
    # SpuÅ¡tÄ›nÃ­ Next.js
    exec npm start
}

# Kontrola argumentÅ¯
case "${1:-full}" in
    "backend")
        start_backend
        ;;
    "frontend")
        start_frontend
        ;;
    "full")
        echo "ğŸš€ Starting both backend and frontend..."
         
         # SpuÅ¡tÄ›nÃ­ backend na pozadÃ­
         start_backend &
         BACKEND_PID=$!
         
         # ÄŒekÃ¡nÃ­ na spuÅ¡tÄ›nÃ­ backend
         echo "â³ Waiting for backend to start..."
         sleep 10
         
         # Kontrola backend health
         if curl -f http://localhost:8000/api/health > /dev/null 2>&1; then
             echo "âœ… Backend is healthy"
         else
             echo "âŒ Backend health check failed"
         fi
         
         # SpuÅ¡tÄ›nÃ­ frontend
         start_frontend &
         FRONTEND_PID=$!
         
         # ÄŒekÃ¡nÃ­ na oba procesy
         wait $BACKEND_PID $FRONTEND_PID
        ;;
    "test")
        echo "ğŸ§ª Running tests..."
        cd /app/backend
        python -c "
print('ğŸ” Testing imports...')
try:
    import torch
    print(f'âœ… PyTorch: {torch.__version__}')
    print(f'âœ… CUDA Available: {torch.cuda.is_available()}')
    if torch.cuda.is_available():
        print(f'âœ… CUDA Device: {torch.cuda.get_device_name()}')
except Exception as e:
    print(f'âŒ PyTorch error: {e}')

try:
    import diffusers
    print(f'âœ… Diffusers: {diffusers.__version__}')
except Exception as e:
    print(f'âŒ Diffusers error: {e}')

try:
    import transformers
    print(f'âœ… Transformers: {transformers.__version__}')
except Exception as e:
    print(f'âŒ Transformers error: {e}')

try:
    from model_manager import model_manager
    models = model_manager.get_available_models()
    print(f'âœ… Model Manager: Found {len(models)} models')
except Exception as e:
    print(f'âŒ Model Manager error: {e}')

try:
    from ai_pipeline import ai_processor
    stats = ai_processor.get_performance_stats()
    print(f'âœ… AI Pipeline: {stats["device"]}')
except Exception as e:
    print(f'âŒ AI Pipeline error: {e}')

print('ğŸ‰ Test completed!')
"
        ;;
    *)
        echo "Usage: $0 {backend|frontend|full|test}"
        echo "  backend  - Start only backend API (default)"
        echo "  frontend - Start only frontend"
        echo "  full     - Start both backend and frontend"
        echo "  test     - Run system tests"
        exit 1
        ;;
esac